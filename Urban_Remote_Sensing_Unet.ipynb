{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from skimage.transform import resize\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from skimage.transform import resize\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
    "    \"\"\"\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    \n",
    "    The jaccard distance loss is usefull for unbalanced datasets. This has been\n",
    "    shifted so it converges on 0 and is smoothed to avoid exploding or disapearing\n",
    "    gradient.\n",
    "    \n",
    "    Ref: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    \n",
    "    @url: https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "    @author: wassname\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(r'.\\data.npy')\n",
    "labels = np.load(r'.\\multi_mask.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 256\n",
    "ny = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 256, 256, 1)\n",
      "(33, 256, 256, 4)\n",
      "(33, 256, 256, 5)\n"
     ]
    }
   ],
   "source": [
    "ndvi = np.expand_dims((data[:,:,:,0]-data[:,:,:,1])/(data[:,:,:,0]+data[:,:,:,1]), axis=-1)\n",
    "print(np.shape(ndvi))\n",
    "print(np.shape(data))\n",
    "data = np.append(data, ndvi, axis=-1)\n",
    "print(np.shape(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization across all samples (band specific)\n",
    "data_scaled = np.zeros(np.shape(data))\n",
    "data_normalized = np.zeros((np.shape(data)))\n",
    "for i in range(np.shape(data)[-1]):\n",
    "    data_mean = np.mean(data[:,:,:,i])\n",
    "    data_std = np.std(data[:,:,:,i])\n",
    "    data_scaled[:,:,:,i] = (data[:,:,:,i]-data_mean)/data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_scaled\n",
    "y = labels[:,:,:,:-1] # exclude clutter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 256, 256, 5)\n",
      "(33, 256, 256, 5)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(x))\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10.181555672321565\n",
      "11.160009094659108\n"
     ]
    }
   ],
   "source": [
    "print(np.min(x))\n",
    "print(np.max(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.20, shuffle=True, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation on just the training data\n",
    "## Image Augmentation\n",
    "# Vertical Image\n",
    "#Vx = [np.flip(x, axis=1) for x in x_train]\n",
    "#Vy = [np.flip(x, axis=1) for x in y_train]\n",
    "\n",
    "# Horizontal Image\n",
    "#Hx = [np.flip(x, axis=2) for x in x_train]\n",
    "#Hy = [np.flip(x, axis=2) for x in y_train]\n",
    "\n",
    "# Horizontal Vertical Image\n",
    "#HVx = [np.flip(x, axis=2) for x in Vx]\n",
    "#HVy = [np.flip(x, axis=2) for x in Vy]\n",
    "\n",
    "# Appending the augmented image and mask to the main dataset.\n",
    "#x_train = np.append(x_train, Vx, axis=0)\n",
    "#y_train = np.append(y_train, Vy, axis=0)\n",
    "\n",
    "#x_train = np.append(x_train, Hx, axis=0)\n",
    "#y_train = np.append(y_train, Hy, axis=0)\n",
    "\n",
    "#x_train = np.append(x_train, HVx, axis=0)\n",
    "#y_train = np.append(y_train, HVy, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_block(input_tensor, n_filters, kernel_size=3, l2_lambda=0, batchnorm=True):\n",
    "    # first layer\n",
    "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
    "               padding=\"same\", kernel_regularizer=regularizers.l2(l2_lambda))(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    # second layer\n",
    "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
    "               padding=\"same\", kernel_regularizer=regularizers.l2(l2_lambda))(x)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet(input_img, n_filters=32, dropout=0.5, batchnorm=True):\n",
    "    # contracting path\n",
    "    c1 = conv2d_block(input_img, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
    "    p1 = MaxPooling2D((2, 2)) (c1)\n",
    "    p1 = Dropout(dropout*0.5)(p1)\n",
    "\n",
    "    c2 = conv2d_block(p1, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
    "    p2 = MaxPooling2D((2, 2)) (c2)\n",
    "    p2 = Dropout(dropout)(p2)\n",
    "\n",
    "    c3 = conv2d_block(p2, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
    "    p3 = MaxPooling2D((2, 2)) (c3)\n",
    "    p3 = Dropout(dropout)(p3)\n",
    "\n",
    "    c4 = conv2d_block(p3, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "    p4 = Dropout(dropout)(p4)\n",
    "    \n",
    "    c5 = conv2d_block(p4, n_filters=n_filters*16, kernel_size=3, batchnorm=batchnorm)\n",
    "    \n",
    "    # expansive path\n",
    "    u6 = Conv2DTranspose(n_filters*8, (3, 3), strides=(2, 2), padding='same') (c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    u6 = Dropout(dropout)(u6)\n",
    "    c6 = conv2d_block(u6, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    u7 = Conv2DTranspose(n_filters*4, (3, 3), strides=(2, 2), padding='same') (c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    u7 = Dropout(dropout)(u7)\n",
    "    c7 = conv2d_block(u7, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    u8 = Conv2DTranspose(n_filters*2, (3, 3), strides=(2, 2), padding='same') (c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    u8 = Dropout(dropout)(u8)\n",
    "    c8 = conv2d_block(u8, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    u9 = Conv2DTranspose(n_filters*1, (3, 3), strides=(2, 2), padding='same') (c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    u9 = Dropout(dropout)(u9)\n",
    "    c9 = conv2d_block(u9, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
    "    \n",
    "    outputs = Conv2D(5, (1, 1), activation='sigmoid') (c9)\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "img (InputLayer)                (None, 256, 256, 5)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 32) 1472        img[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256, 256, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 256, 256, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 256, 256, 32) 9248        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 256, 256, 32) 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 256, 256, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 32) 0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128, 128, 32) 0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 64) 18496       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128, 128, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 128, 128, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 64) 36928       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 128, 128, 64) 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 128, 128, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 64)   0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64, 64, 64)   0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 128)  73856       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 64, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64, 64, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 128)  147584      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 64, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 64, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 128)  0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32, 32, 128)  0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 256)  295168      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 256)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 256)  590080      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 256)  1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 256)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 256)  0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 16, 16, 256)  0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 512)  1180160     dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 512)  2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 512)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 512)  2359808     activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 512)  2048        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 512)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 256)  1179904     activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 512)  0           conv2d_transpose_1[0][0]         \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32, 32, 512)  0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 256)  1179904     dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 256)  1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 256)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 256)  590080      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 256)  1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 256)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 128)  295040      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 256)  0           conv2d_transpose_2[0][0]         \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 64, 64, 256)  0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 128)  295040      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 64, 64, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 64, 64, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 128)  147584      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 128)  512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 128)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 64) 73792       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 128 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 128, 128, 128 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 128, 128, 64) 73792       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 128, 128, 64) 256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 128, 128, 64) 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 64) 36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 64) 256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 64) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 256, 256, 32) 18464       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 256, 256, 64) 0           conv2d_transpose_4[0][0]         \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 256, 256, 64) 0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 256, 256, 32) 18464       dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 256, 256, 32) 128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 256, 256, 32) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 256, 256, 32) 9248        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 256, 256, 32) 128         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 256, 256, 32) 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 256, 256, 5)  165         activation_18[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,642,981\n",
      "Trainable params: 8,637,093\n",
      "Non-trainable params: 5,888\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_img = Input((256, 256, 5), name='img')\n",
    "model = get_unet(input_img, n_filters=32, dropout=0.5, batchnorm=True)\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=[\"categorical_accuracy\", f1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint('model-test-4.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26 samples, validate on 7 samples\n",
      "Epoch 1/100\n",
      "26/26 [==============================] - 38s 1s/step - loss: 1.3190 - categorical_accuracy: 0.4514 - f1: 0.4598 - val_loss: 1.1368 - val_categorical_accuracy: 0.5041 - val_f1: 0.5298\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.13677, saving model to model-test-4.h5\n",
      "Epoch 2/100\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.1371 - categorical_accuracy: 0.6038 - f1: 0.5521 - val_loss: 1.0658 - val_categorical_accuracy: 0.5093 - val_f1: 0.5394\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.13677 to 1.06579, saving model to model-test-4.h5\n",
      "Epoch 3/100\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.0224 - categorical_accuracy: 0.6774 - f1: 0.6014 - val_loss: 1.2138 - val_categorical_accuracy: 0.5693 - val_f1: 0.5483\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.06579\n",
      "Epoch 4/100\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.8982 - categorical_accuracy: 0.7204 - f1: 0.6654 - val_loss: 1.1211 - val_categorical_accuracy: 0.5480 - val_f1: 0.5820\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.06579\n",
      "Epoch 5/100\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.7960 - categorical_accuracy: 0.7477 - f1: 0.7136 - val_loss: 1.1040 - val_categorical_accuracy: 0.5909 - val_f1: 0.6065\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.06579\n",
      "Epoch 6/100\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.7149 - categorical_accuracy: 0.7646 - f1: 0.7475 - val_loss: 1.1866 - val_categorical_accuracy: 0.5801 - val_f1: 0.5990\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.06579\n",
      "Epoch 7/100\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.6479 - categorical_accuracy: 0.7823 - f1: 0.7707 - val_loss: 9.3791 - val_categorical_accuracy: 0.2709 - val_f1: 0.2807\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.06579\n",
      "Epoch 8/100\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.5865 - categorical_accuracy: 0.8077 - f1: 0.7954 - val_loss: 1.6716 - val_categorical_accuracy: 0.5111 - val_f1: 0.5262\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.06579\n",
      "Epoch 9/100\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.5698 - categorical_accuracy: 0.8153 - f1: 0.8028 - val_loss: 0.9955 - val_categorical_accuracy: 0.5958 - val_f1: 0.6169\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.06579 to 0.99548, saving model to model-test-4.h5\n",
      "Epoch 10/100\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.5616 - categorical_accuracy: 0.8180 - f1: 0.8063 - val_loss: 0.8540 - val_categorical_accuracy: 0.6480 - val_f1: 0.6672\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.99548 to 0.85398, saving model to model-test-4.h5\n",
      "Epoch 11/100\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.5551 - categorical_accuracy: 0.8201 - f1: 0.8087 - val_loss: 0.7302 - val_categorical_accuracy: 0.7072 - val_f1: 0.7198\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.85398 to 0.73023, saving model to model-test-4.h5\n",
      "Epoch 12/100\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.5474 - categorical_accuracy: 0.8226 - f1: 0.8116 - val_loss: 0.7075 - val_categorical_accuracy: 0.7119 - val_f1: 0.7261\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.73023 to 0.70754, saving model to model-test-4.h5\n",
      "Epoch 13/100\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.5393 - categorical_accuracy: 0.8254 - f1: 0.8149 - val_loss: 0.6809 - val_categorical_accuracy: 0.7238 - val_f1: 0.7369\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.70754 to 0.68086, saving model to model-test-4.h5\n",
      "Epoch 14/100\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.5324 - categorical_accuracy: 0.8272 - f1: 0.8175 - val_loss: 0.7211 - val_categorical_accuracy: 0.7050 - val_f1: 0.7200\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.68086\n",
      "Epoch 15/100\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5257 - categorical_accuracy: 0.8293 - f1: 0.8191 - val_loss: 0.6894 - val_categorical_accuracy: 0.7200 - val_f1: 0.7344\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.68086\n",
      "Epoch 16/100\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5198 - categorical_accuracy: 0.8308 - f1: 0.8213 - val_loss: 0.7047 - val_categorical_accuracy: 0.7127 - val_f1: 0.7285\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.68086\n",
      "Epoch 17/100\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5124 - categorical_accuracy: 0.8333 - f1: 0.8240 - val_loss: 0.7192 - val_categorical_accuracy: 0.7054 - val_f1: 0.7214\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.68086\n",
      "Epoch 18/100\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5060 - categorical_accuracy: 0.8354 - f1: 0.8263 - val_loss: 0.7297 - val_categorical_accuracy: 0.7024 - val_f1: 0.7184\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.68086\n",
      "Epoch 19/100\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5003 - categorical_accuracy: 0.8370 - f1: 0.8286 - val_loss: 0.7184 - val_categorical_accuracy: 0.7067 - val_f1: 0.7226\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.68086\n",
      "Epoch 20/100\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4987 - categorical_accuracy: 0.8375 - f1: 0.8290 - val_loss: 0.7119 - val_categorical_accuracy: 0.7095 - val_f1: 0.7249\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.68086\n",
      "Epoch 21/100\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4982 - categorical_accuracy: 0.8386 - f1: 0.8296 - val_loss: 0.7105 - val_categorical_accuracy: 0.7104 - val_f1: 0.7255\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.68086\n",
      "Epoch 22/100\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4988 - categorical_accuracy: 0.8370 - f1: 0.8290 - val_loss: 0.7117 - val_categorical_accuracy: 0.7100 - val_f1: 0.7251\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.68086\n",
      "Epoch 23/100\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4977 - categorical_accuracy: 0.8385 - f1: 0.8296 - val_loss: 0.7137 - val_categorical_accuracy: 0.7092 - val_f1: 0.7243\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.68086\n",
      "Epoch 00023: early stopping\n"
     ]
    }
   ],
   "source": [
    "results = model.fit(x_train, y_train, batch_size=1, epochs=100, callbacks=callbacks,\n",
    "                   validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"log_loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_weights('model-test-4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set (this must be equals to the best log_loss)\n",
    "model.evaluate(x_val, y_val, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on train, val and test\n",
    "preds_train = model.predict(x_train, verbose=1)\n",
    "preds_val = model.predict(x_val, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(preds_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold predictions\n",
    "\n",
    "preds_train_t = (preds_train > 0.5).astype(np.uint8)\n",
    "preds_val_t = (preds_val > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(x_train[0,:,:,0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(X, y, preds, binary_preds, ix=None):\n",
    "    if ix is None:\n",
    "        ix = random.randint(0, len(X))\n",
    "\n",
    "    has_mask = y[ix].max() > 0\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(20, 10))\n",
    "    r_band = (X[ix,:,:,0]-np.min(X[ix,:,:,0]))/(np.max(X[ix,:,:,0])-np.min(X[ix,:,:,0]))\n",
    "    g_band = (X[ix,:,:,1]-np.min(X[ix,:,:,1]))/(np.max(X[ix,:,:,1])-np.min(X[ix,:,:,1]))\n",
    "    b_band = (X[ix,:,:,2]-np.min(X[ix,:,:,2]))/(np.max(X[ix,:,:,2])-np.min(X[ix,:,:,2]))\n",
    "    RGB = np.stack((r_band, g_band, b_band), axis=-1)\n",
    "    print(np.shape(RGB))\n",
    "    im0 = ax[0,0].imshow(RGB)\n",
    "    #if has_mask:\n",
    "        #ax[0].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n",
    "    fig.colorbar(im0, ax=ax[0,0], fraction=0.046, pad=0.04)\n",
    "    ax[0,0].set_title('Remote Sensing Image')\n",
    "\n",
    "    im1 = ax[0,1].imshow(y[ix,:,:,0].squeeze())\n",
    "    ax[0,1].set_title('Impervious Surfaces')\n",
    "    fig.colorbar(im1, ax=ax[0,1], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    im2 = ax[1,0].imshow(preds[ix,:,:,0].squeeze(), vmin=0, vmax=1)\n",
    "    #if has_mask:\n",
    "        #ax[2].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n",
    "    fig.colorbar(im2, ax=ax[1,0], fraction=0.046, pad=0.04)\n",
    "    ax[1,0].set_title('Impervious Surfaces Predicted')\n",
    "    \n",
    "    im3 = ax[1,1].imshow(binary_preds[ix,:,:,0].squeeze(), vmin=0, vmax=1)\n",
    "    #if has_mask:\n",
    "        #ax[3].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n",
    "    fig.colorbar(im3, ax=ax[1,1], fraction=0.046, pad=0.04)\n",
    "    ax[1,1].set_title('Impervious Surfaces Predicted (Binary)')\n",
    "    fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_all(X, y, preds, binary_preds, ix=None, filename='Sample.png'):\n",
    "    import matplotlib\n",
    "\n",
    "    matplotlib.rc('xtick', labelsize=20) \n",
    "    matplotlib.rc('ytick', labelsize=20) \n",
    "    if ix is None:\n",
    "        ix = random.randint(0, len(X))\n",
    "\n",
    "    has_mask = y[ix].max() > 0\n",
    "\n",
    "    fig, ax = plt.subplots(5, 2, sharex=True, sharey=True, figsize=(20, 50))\n",
    "    r_band = (X[ix,:,:,0]-np.min(X[ix,:,:,0]))/(np.max(X[ix,:,:,0])-np.min(X[ix,:,:,0]))\n",
    "    g_band = (X[ix,:,:,1]-np.min(X[ix,:,:,1]))/(np.max(X[ix,:,:,1])-np.min(X[ix,:,:,1]))\n",
    "    b_band = (X[ix,:,:,2]-np.min(X[ix,:,:,2]))/(np.max(X[ix,:,:,2])-np.min(X[ix,:,:,2]))\n",
    "    RGB = np.stack((r_band, g_band, b_band), axis=-1)\n",
    "\n",
    "    im0 = ax[0,0].imshow(RGB)\n",
    "    #if has_mask:\n",
    "        #ax[0].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n",
    "\n",
    "    ax[0,0].set_title('Remote Sensing Image', fontsize=30)\n",
    "    \n",
    "        \n",
    "    im1 = ax[0,1].imshow(X[ix,:,:,4].squeeze(), cmap='gray')\n",
    "\n",
    "    ax[0,1].set_title('NDVI', fontsize=30)\n",
    "    \n",
    "    im2 = ax[1,0].imshow(X[ix,:,:,3].squeeze(), cmap='gray')\n",
    "\n",
    "    ax[1,0].set_title('Filtered DSM', fontsize=30)\n",
    "    \n",
    "    total_mask = np.zeros((256, 256, 3))\n",
    "    for i in range(256):\n",
    "        for j in range(256):\n",
    "            # impervious surface\n",
    "            if(y[ix,i,j,0]==1):\n",
    "                total_mask[i,j,0]=1\n",
    "                total_mask[i,j,1]=1\n",
    "                total_mask[i,j,2]=1\n",
    "            # building\n",
    "            elif(y[ix,i,j,1]==1):\n",
    "                total_mask[i,j,0]=0\n",
    "                total_mask[i,j,1]=0\n",
    "                total_mask[i,j,2]=1\n",
    "            # low vegetation\n",
    "            elif(y[ix,i,j,2]==1):\n",
    "                total_mask[i,j,0]=0\n",
    "                total_mask[i,j,1]=1\n",
    "                total_mask[i,j,2]=1\n",
    "            # tree\n",
    "            elif(y[ix,i,j,3]==1):\n",
    "                total_mask[i,j,0]=0\n",
    "                total_mask[i,j,1]=1\n",
    "                total_mask[i,j,2]=0\n",
    "            # car\n",
    "            elif(y[ix,i,j,4]==1):\n",
    "                total_mask[i,j,0]=1\n",
    "                total_mask[i,j,1]=0\n",
    "                total_mask[i,j,2]=0\n",
    "\n",
    "                \n",
    "    im3 = ax[1,1].imshow(total_mask)\n",
    "    ax[1,1].set_title('Image Mask', fontsize=30)\n",
    "    \n",
    "    im4 = ax[2,0].imshow(binary_preds[ix,:,:,0].squeeze(), vmin=0, vmax=1)\n",
    "\n",
    "    ax[2,0].set_title('Impervious Surface Predicted (Binary)', fontsize=30)\n",
    "    \n",
    "    im5 = ax[2,1].imshow(binary_preds[ix,:,:,1].squeeze(), vmin=0, vmax=1)\n",
    "\n",
    "    ax[2,1].set_title('Building Predicted (Binary)', fontsize=30)\n",
    "    \n",
    "    im6 = ax[3,0].imshow(binary_preds[ix,:,:,2].squeeze(), vmin=0, vmax=1)\n",
    "\n",
    "    ax[3,0].set_title('Low Vegetation Predicted (Binary)', fontsize=30)\n",
    "    \n",
    "    im7 = ax[3,1].imshow(binary_preds[ix,:,:,3].squeeze(), vmin=0, vmax=1)\n",
    "\n",
    "    ax[3,1].set_title('Trees Predicted (Binary)', fontsize=30)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    \n",
    "    im8 = ax[4,0].imshow(binary_preds[ix,:,:,4].squeeze(), vmin=0, vmax=1)\n",
    "\n",
    "    ax[4,0].set_title('Cars Predicted (Binary)', fontsize=30)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    \n",
    "    total_preds = np.zeros((256,256,3))\n",
    "    for i in range(256):\n",
    "        for j in range(256):\n",
    "            # impervious surface\n",
    "            if(np.max(preds[ix,i,j])==preds[ix,i,j,0]):\n",
    "                total_preds[i,j,0]=1\n",
    "                total_preds[i,j,1]=1\n",
    "                total_preds[i,j,2]=1\n",
    "            # building\n",
    "            elif(np.max(preds[ix,i,j])==preds[ix,i,j,1]):\n",
    "                total_preds[i,j,0]=0\n",
    "                total_preds[i,j,1]=0\n",
    "                total_preds[i,j,2]=1\n",
    "            # low vegetation\n",
    "            elif(np.max(preds[ix,i,j])==preds[ix,i,j,2]):\n",
    "                total_preds[i,j,0]=0\n",
    "                total_preds[i,j,1]=1\n",
    "                total_preds[i,j,2]=1\n",
    "            # tree\n",
    "            elif(np.max(preds[ix,i,j])==preds[ix,i,j,3]):\n",
    "                total_preds[i,j,0]=0\n",
    "                total_preds[i,j,1]=1\n",
    "                total_preds[i,j,2]=0\n",
    "            # car\n",
    "            elif(np.max(preds[ix,i,j])==preds[ix,i,j,4]):\n",
    "                total_preds[i,j,0]=1\n",
    "                total_preds[i,j,1]=0\n",
    "                total_preds[i,j,2]=0\n",
    "    im9 = ax[4,1].imshow(total_preds)\n",
    "\n",
    "    ax[4,1].set_title('All Maximum Class Predictions', fontsize=30)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=22)\n",
    "    fig.tight_layout();\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, sharex=True, sharey=True, figsize=(8,6))\n",
    "im = ax.imshow(x_train[0,:,:,3].squeeze(), cmap='gray')\n",
    "ax.set_title('Filtered DSM', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('filteredDSM.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if training data looks all right\n",
    "plot_sample_all(x_val, y_val, preds_val, preds_val_t, ix=2, filename='test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if valid data looks all right\n",
    "plot_sample(x_val, y_val, preds_val, preds_val_t, ix=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_total = np.concatenate((x_train, x_val), axis=0)\n",
    "y_total = np.concatenate((y_train, y_val), axis=0)\n",
    "pred_total = np.concatenate((preds_train_t, preds_val_t), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_total.ravel(), pred_total.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_roc_metrics(y_real, y_predict):\n",
    "\n",
    "    c_matrix = confusion_matrix(y_real.ravel(), y_predict.ravel())\n",
    "    f1 = f1_score(y_real.ravel(), y_predict.ravel())\n",
    "    recall = recall_score(y_real.ravel(), y_predict.ravel())\n",
    "    precision = precision_score(y_real.ravel(), y_predict.ravel())\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(c_matrix)\n",
    "    print(\"F1 score: {:.4f}\".format(f1))\n",
    "    print(\"Recall score: {:.4f}\".format(recall))\n",
    "    print(\"Precision score: {:.4f}\".format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_roc_metrics(y_val, preds_val_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
